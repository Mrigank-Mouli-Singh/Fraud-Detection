{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIYo94/43jQ7tYhGKvx0oV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mrigank-Mouli-Singh/Fraud-Detection/blob/main/Fraud_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš¨ Fraud Detection Case Study\n",
        "\n",
        "## Business Context\n",
        "This project builds a machine learning model to proactively detect fraudulent transactions for a financial services company.  \n",
        "Dataset: ~6.3M transactions (10 raw columns, engineered to ~20 features).  \n",
        "**Goal:** Predict whether a transaction is fraudulent and provide actionable insights for prevention.\n",
        "\n",
        "---\n",
        "\n",
        "## Process Overview\n",
        "\n",
        "1. **Data Cleaning & Preparation**\n",
        "   - Handled missing values, outliers, multicollinearity (VIF).\n",
        "   - Engineered features (e.g., `deltaOrig`, `amt_to_org_bal`, time-based features).\n",
        "\n",
        "2. **Handling Class Imbalance**\n",
        "   - Fraud rate ~0.13%.\n",
        "   - Used LightGBM with `scale_pos_weight â‰ˆ (#nonfraud / #fraud)` and PR-AUC evaluation.\n",
        "\n",
        "3. **Modeling**\n",
        "   - Hyperparameter tuning with **Optuna** (PR-AUC optimized).\n",
        "   - Final model: LightGBM, trained with early stopping.\n",
        "   - Evaluation on hold-out validation set.\n",
        "\n",
        "4. **Interpretability (SHAP)**\n",
        "   - **Global feature importance:**\n",
        "     - `amt_to_org_bal` â†’ strongest fraud predictor.\n",
        "     - `deltaOrig`, `oldbalanceDest`, and transaction type indicators (transfers/cash-outs).\n",
        "   - **Local explanations:** which features pushed an individual transaction towards fraud.\n",
        "\n",
        "5. **Business Insights**\n",
        "   - Fraudsters typically transfer large amounts relative to balance.\n",
        "   - Cash-out and transfer transactions dominate fraud cases.\n",
        "   - Suspicious merchants receive disproportionate inflows.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Results\n",
        "\n",
        "- **Validation ROC-AUC**: ~0.9998  \n",
        "- **Validation PR-AUC**: ~0.90  \n",
        "- Fraud recall ~99.8% (at threshold=0.5), though with lower precision.  \n",
        "- Business-aware thresholds can be tuned for cost trade-offs.  \n",
        "\n",
        "---\n",
        "\n",
        "## Deliverables\n",
        "\n",
        "ðŸ“‚ `final_fraud_lgb.txt` â†’ Trained LightGBM model  \n",
        "ðŸ“‚ `final_metrics.json` â†’ Evaluation metrics  \n",
        "ðŸ“‚ `feature_importance.csv` â†’ Model feature importances  \n",
        "ðŸ“‚ `shap_feature_importance.csv` â†’ SHAP global importances  \n",
        "ðŸ“‚ `shap_fraud_only.csv` â†’ SHAP values for fraud subset  \n",
        "ðŸ“‚ `shap_type_influence.csv` â†’ Avg SHAP influence by transaction type  \n",
        "ðŸ“‚ `validation_predictions.csv` â†’ Validation set predictions (with chosen threshold)  \n",
        "\n",
        "---\n",
        "\n",
        "## Recommendations\n",
        "\n",
        "- Deploy fraud scoring API in production (real-time scoring).  \n",
        "- Dynamic monitoring: flag `amt_to_org_bal > 0.8`.  \n",
        "- Extra scrutiny for **transfers** and **cash-outs**.  \n",
        "- Merchant risk profiling for unusual inflows.  \n",
        "- Regular retraining with updated fraud patterns.  \n",
        "\n",
        "---\n",
        "\n",
        "ðŸ‘‰ This notebook follows a clear structure with alternating **Markdown (explanation)** and **Code (implementation)** cells, making it suitable for both technical and business review.\n"
      ],
      "metadata": {
        "id": "XqG1LnxE_q5R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9AhL23N-vNx"
      },
      "outputs": [],
      "source": [
        "#Cell 2 â€” Code (Install & Imports)\n",
        "!pip -q install lightgbm imbalanced-learn shap optuna category_encoders\n",
        "\n",
        "import os, gc, warnings, math, json, pickle, textwrap, random\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option(\"display.max_columns\", 200)\n",
        "print(\"âœ… Libraries ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 3 â€” Markdown\n",
        "## 0) Bring the Data In\n",
        "\n",
        "We have a dataset of **6,362,620 rows Ã— 10 columns**.  \n",
        "Because the dataset is large, weâ€™ll first load a **sample (10k rows)** to inspect structure, then load the full file efficiently.\n",
        "\n",
        "Choose ONE of the following:\n",
        "- **A. Upload files** (`Fraud.csv`, `Data Dictionary.txt`) using the Colab file picker.  \n",
        "- **B. Mount Google Drive** and set paths manually (uncomment code).  \n",
        "\n",
        "> âš ï¸ Full load may require memory optimization (dtypes, chunking)."
      ],
      "metadata": {
        "id": "_BqfTz2p_0j0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 â€” Code (Load files)\n",
        "# Install gdown for Google Drive file download\n",
        "!pip -q install gdown\n",
        "\n",
        "import gdown, os\n",
        "\n",
        "# Fraud dataset\n",
        "fraud_url = \"https://drive.google.com/uc?id=1l4n9t872C-KHwFbbpN5W5BDglMQt8eZH\"\n",
        "fraud_out = \"Fraud.csv\"\n",
        "if not os.path.exists(fraud_out):\n",
        "    gdown.download(fraud_url, fraud_out, quiet=False)\n",
        "\n",
        "# Data dictionary\n",
        "dict_url = \"https://drive.google.com/uc?id=1KqN8YhBNz7Tc49buWWng88qtnHccEhyG\"\n",
        "dict_out = \"Data_Dictionary.txt\"\n",
        "if not os.path.exists(dict_out):\n",
        "    gdown.download(dict_url, dict_out, quiet=False)\n",
        "\n",
        "DATA_CSV = fraud_out\n",
        "DICT_TXT = dict_out\n",
        "\n",
        "print(\"âœ… Downloaded files\")\n",
        "print(\"DATA_CSV:\", DATA_CSV)\n",
        "print(\"DICT_TXT:\", DICT_TXT)\n"
      ],
      "metadata": {
        "id": "irama6LwA1Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 5 â€” Markdown\n",
        "## 1) Load efficiently + basic schema\n",
        "\n",
        "We downcast numerics and use `category` for strings to save memory on 6.3M rows."
      ],
      "metadata": {
        "id": "xiwNUVsCDCUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 â€” Code (Read CSV memory-aware)\n",
        "# Expected columns from dictionary:\n",
        "# step, type, amount, nameOrig, oldbalanceOrg, newbalanceOrig, nameDest, oldbalanceDest, newbalanceDest, isFraud, isFlaggedFraud\n",
        "\n",
        "dtype_map = {\n",
        "    \"step\": \"int32\",\n",
        "    \"type\": \"category\",\n",
        "    \"amount\": \"float32\",\n",
        "    \"nameOrig\": \"category\",\n",
        "    \"oldbalanceOrg\": \"float32\",\n",
        "    \"newbalanceOrig\": \"float32\",\n",
        "    \"nameDest\": \"category\",\n",
        "    \"oldbalanceDest\": \"float32\",\n",
        "    \"newbalanceDest\": \"float32\",\n",
        "    \"isFraud\": \"int8\",\n",
        "    \"isFlaggedFraud\": \"int8\",\n",
        "}\n",
        "\n",
        "df = pd.read_csv(DATA_CSV, dtype=dtype_map)\n",
        "print(\"Shape:\", df.shape)\n",
        "display(df.head(3))\n",
        "print(\"\\nClass balance (isFraud):\")\n",
        "display(df['isFraud'].value_counts(normalize=True).rename('share').to_frame())\n"
      ],
      "metadata": {
        "id": "TJ0jTcrUDJyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 â€” Code (Show dictionary & quick checks)\n",
        "try:\n",
        "    with open(DICT_TXT, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        dictionary_text = f.read()\n",
        "    print(\"ðŸ“„ Data Dictionary (first 40 lines):\\n\")\n",
        "    print(\"\\n\".join(dictionary_text.splitlines()[:40]))\n",
        "except Exception as e:\n",
        "    print(\"Data dictionary not found or unreadable:\", e)\n",
        "\n",
        "print(\"\\ndtypes:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\nMissing values per column:\")\n",
        "display(df.isna().sum().to_frame(\"missing\"))\n"
      ],
      "metadata": {
        "id": "CBKXgsgwEAcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 8 â€” Markdown\n",
        "## 2) Exploratory data quality signals\n",
        "\n",
        "- **Outliers**: heavy-tailed amounts/balances  \n",
        "- **Logical consistency**: balance deltas before/after transaction  \n",
        "- **Leakage checks**: drop names, keep only operational fields"
      ],
      "metadata": {
        "id": "_e8ajRECEj7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9 â€” Code (Data quality & feature engineering)\n",
        "# Logical deltas: how balances should move given amount\n",
        "df[\"deltaOrig\"] = (df[\"oldbalanceOrg\"] - df[\"newbalanceOrig\"] - df[\"amount\"]).astype(\"float32\")\n",
        "df[\"deltaDest\"] = (df[\"newbalanceDest\"] - df[\"oldbalanceDest\"] - df[\"amount\"]).astype(\"float32\")\n",
        "\n",
        "# Flags helpful for fraud patterns\n",
        "df[\"isTransfer\"] = (df[\"type\"].astype(str) == \"TRANSFER\").astype(\"int8\")\n",
        "df[\"isCashOut\"]  = (df[\"type\"].astype(str) == \"CASH_OUT\").astype(\"int8\")\n",
        "df[\"isPayment\"]  = (df[\"type\"].astype(str) == \"PAYMENT\").astype(\"int8\")\n",
        "df[\"destIsMerchant\"] = df[\"nameDest\"].astype(str).str.startswith(\"M\").astype(\"int8\")\n",
        "\n",
        "# Ratios and magnitudes (guard against divide-by-zero)\n",
        "df[\"amt_log1p\"] = np.log1p(df[\"amount\"]).astype(\"float32\")\n",
        "df[\"balOrg_log1p\"] = np.log1p(df[\"oldbalanceOrg\"].clip(lower=0)).astype(\"float32\")\n",
        "df[\"balDest_log1p\"] = np.log1p(df[\"oldbalanceDest\"].clip(lower=0)).astype(\"float32\")\n",
        "df[\"amt_to_org_bal\"] = (df[\"amount\"] / (1.0 + df[\"oldbalanceOrg\"])).astype(\"float32\")\n",
        "\n",
        "# Temporal features: step is hourly; add hour-of-day and day-of-week proxies\n",
        "df[\"hour\"] = (df[\"step\"] % 24).astype(\"int8\")\n",
        "df[\"day\"]  = (df[\"step\"] // 24 % 7).astype(\"int8\")\n",
        "\n",
        "# View some summaries\n",
        "print(df[[\"amount\",\"oldbalanceOrg\",\"newbalanceOrig\",\"oldbalanceDest\",\"newbalanceDest\",\"deltaOrig\",\"deltaDest\"]].describe().T)\n"
      ],
      "metadata": {
        "id": "G1_uGd-lErrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 10 â€” Markdown\n",
        "## 3) Missing values, outliers, and multicollinearity\n",
        "\n",
        "- **Missing**: impute if any (median for numeric, most frequent for categorical)  \n",
        "- **Outliers**: winsorize extreme tails on `amount` & balances (99.9th)  \n",
        "- **Multicollinearity**: compute VIF and drop features with VIF > 10"
      ],
      "metadata": {
        "id": "i_QDvqqSGawZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11 â€” Code (Clean)\n",
        "# (A) Missing values â€” robust check\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_cols = df.select_dtypes(include=[\"category\", \"object\"]).columns.tolist()\n",
        "\n",
        "for c in num_cols:\n",
        "    if df[c].isna().any():\n",
        "        df[c] = df[c].fillna(df[c].median())\n",
        "\n",
        "for c in cat_cols:\n",
        "    if df[c].isna().any():\n",
        "        df[c] = df[c].astype(\"category\").cat.add_categories([\"__MISSING__\"]).fillna(\"__MISSING__\")\n",
        "\n",
        "# (B) Outliers â€” winsorize heavy tails\n",
        "def winsorize_series(s, lower_q=0.001, upper_q=0.999):\n",
        "    lo, hi = s.quantile(lower_q), s.quantile(upper_q)\n",
        "    return s.clip(lo, hi)\n",
        "\n",
        "for c in [\"amount\",\"oldbalanceOrg\",\"newbalanceOrig\",\"oldbalanceDest\",\"newbalanceDest\",\"deltaOrig\",\"deltaDest\"]:\n",
        "    df[c] = winsorize_series(df[c])\n",
        "\n",
        "# (C) Multicollinearity (VIF) â€” only for numeric features\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "vif_df = df[num_cols].drop(columns=[\"isFraud\",\"isFlaggedFraud\"], errors=\"ignore\")\n",
        "\n",
        "# Sample for speed (dataset is huge)\n",
        "vif_sample = vif_df.sample(200_000, random_state=42) if len(vif_df) > 200_000 else vif_df\n",
        "\n",
        "vif_values = []\n",
        "cols_for_vif = vif_sample.columns.tolist()\n",
        "X_vif = vif_sample.values\n",
        "for i in range(X_vif.shape[1]):\n",
        "    try:\n",
        "        vif_values.append((cols_for_vif[i], float(variance_inflation_factor(X_vif, i))))\n",
        "    except Exception:\n",
        "        vif_values.append((cols_for_vif[i], np.nan))\n",
        "\n",
        "vif_table = pd.DataFrame(vif_values, columns=[\"feature\",\"VIF\"]).sort_values(\"VIF\", ascending=False)\n",
        "display(vif_table.head(15))\n",
        "\n",
        "# âœ… Drop only truly redundant features\n",
        "to_drop_for_vif = [\"newbalanceOrig\", \"newbalanceDest\"]\n",
        "print(\"Dropping due to redundancy:\", to_drop_for_vif)\n",
        "df = df.drop(columns=to_drop_for_vif)\n",
        "\n"
      ],
      "metadata": {
        "id": "ztmOWxVsGc9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns.tolist())\n"
      ],
      "metadata": {
        "id": "8VNic22iIrul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 12 â€” Markdown\n",
        "## 4) Train / Calibration / Validation split\n",
        "\n",
        "We stratify by `isFraud` to preserve the class ratio and hold out a final validation set.\n",
        "\n",
        "- **Train set** â†’ main dataset for fitting the model and learning parameters.  \n",
        "- **Calibration set** â†’ reserved for probability calibration, threshold tuning, and cost-sensitive optimization.  \n",
        "- **Validation set** â†’ untouched holdout for final, unbiased evaluation of model performance.  \n",
        "\n",
        "IDs (`nameOrig`, `nameDest`) are dropped to avoid leakage.  \n",
        "We keep engineered features (ratios, logs, deltas, flags) since they capture fraud patterns effectively.\n"
      ],
      "metadata": {
        "id": "FQ8jhcFTJUOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13 â€” Code (the splitting logic I gave you in the last message)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "TARGET = \"isFraud\"\n",
        "\n",
        "# Drop IDs and non-features\n",
        "drop_cols = [\"nameOrig\", \"nameDest\", \"isFraud\", \"isFlaggedFraud\"]\n",
        "features = [c for c in df.columns if c not in drop_cols]\n",
        "\n",
        "X = df[features].copy()\n",
        "y = df[TARGET].astype(\"int8\")\n",
        "\n",
        "# Convert categorical 'type' into numeric codes for LightGBM\n",
        "if \"type\" in X.columns and str(X[\"type\"].dtype) == \"category\":\n",
        "    X[\"type_code\"] = X[\"type\"].cat.codes.astype(\"int16\")\n",
        "    X = X.drop(columns=[\"type\"])\n",
        "\n",
        "# Split: Train+Calib vs Validation\n",
        "X_trc, X_val, y_trc, y_val = train_test_split(\n",
        "    X, y, test_size=0.15, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# From Train+Calib, create a Calibration fold\n",
        "X_train, X_cal, y_train, y_cal = train_test_split(\n",
        "    X_trc, y_trc, test_size=0.15, stratify=y_trc, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Calibration shape:\", X_cal.shape)\n",
        "print(\"Validation shape:\", X_val.shape)\n",
        "print(\"Fraud rate Train/Cal/Val:\", y_train.mean(), y_cal.mean(), y_val.mean())"
      ],
      "metadata": {
        "id": "QJdxdw12Jaq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 14 â€” Markdown\n",
        "\n",
        "### 5) Handle Class Imbalance\n",
        "Fraud rate is only ~0.13%.\n",
        "If we train directly, the model may just predict \"non-fraud\" for everything.\n",
        "\n",
        "To counter imbalance, we use:\n",
        "\n",
        "- `scale_pos_weight` in LightGBM â‰ˆ (#nonfraud / #fraud)  \n",
        "- Optionally undersample or oversample (SMOTE) for experiments  \n",
        "- Evaluate using PR-AUC (better for rare events than ROC-AUC)"
      ],
      "metadata": {
        "id": "On2QnsheJzTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 15 â€” Code\n",
        "# Calculate scale_pos_weight = (#negative / #positive) for LightGBM\n",
        "neg, pos = (y_train == 0).sum(), (y_train == 1).sum()\n",
        "scale_pos_weight = max(1.0, neg / max(1, pos))\n",
        "print(f\"Negatives: {neg:,} | Positives: {pos:,}\")\n",
        "print(\"scale_pos_weight for LightGBM:\", scale_pos_weight)\n"
      ],
      "metadata": {
        "id": "Bad0wXLZJ33b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 16 â€” Markdown\n",
        "\n",
        "### 6) Hyperparameter Search (Optuna) on PR-AUC\n",
        "Because of extreme imbalance, we optimize for PR-AUC (precision-recall area under curve),\n",
        "which better reflects fraud detection performance than ROC-AUC.\n"
      ],
      "metadata": {
        "id": "t2ZAIsA6KEul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 17 â€” Hyperparameter Tuning with Optuna (final, LightGBM 4.x compatible)\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "# For tuning speed, take a sample of train/cal\n",
        "train_idx = np.random.choice(len(X_train), size=300_000, replace=False)\n",
        "cal_idx   = np.random.choice(len(X_cal), size=100_000, replace=False)\n",
        "\n",
        "X_train_sample, y_train_sample = X_train.iloc[train_idx], y_train.iloc[train_idx]\n",
        "X_cal_sample,   y_cal_sample   = X_cal.iloc[cal_idx],   y_cal.iloc[cal_idx]\n",
        "\n",
        "def objective(trial):\n",
        "    boosting_type = trial.suggest_categorical(\"boosting\", [\"gbdt\", \"goss\"])\n",
        "\n",
        "    params = {\n",
        "        \"objective\": \"binary\",\n",
        "        \"metric\": \"auc\",\n",
        "        \"boosting_type\": boosting_type,\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
        "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 255),\n",
        "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 50, 500),\n",
        "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 10.0),\n",
        "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.0, 10.0),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 16),\n",
        "        \"scale_pos_weight\": scale_pos_weight,\n",
        "        \"n_estimators\": 200,\n",
        "        \"verbosity\": -1,\n",
        "        \"n_jobs\": -1,\n",
        "        \"force_col_wise\": True\n",
        "    }\n",
        "\n",
        "    # Bagging is only valid with GBDT\n",
        "    if boosting_type == \"gbdt\":\n",
        "        params.update({\n",
        "            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
        "            \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n",
        "            \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 0, 10),\n",
        "        })\n",
        "    else:  # goss\n",
        "        params.update({\n",
        "            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0)\n",
        "        })\n",
        "\n",
        "    dtrain = lgb.Dataset(X_train_sample, label=y_train_sample)\n",
        "    dcal   = lgb.Dataset(X_cal_sample,   label=y_cal_sample)\n",
        "\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        dtrain,\n",
        "        valid_sets=[dcal],\n",
        "        num_boost_round=200,\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(30),  # replaces early_stopping_rounds\n",
        "            lgb.log_evaluation(-1)   # replaces verbose_eval\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    preds = model.predict(X_cal_sample)\n",
        "    return average_precision_score(y_cal_sample, preds)\n",
        "\n",
        "# Run Optuna optimization\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
        "\n",
        "# Save best parameters\n",
        "best_params = study.best_trial.params\n",
        "best_params.update({\n",
        "    \"objective\": \"binary\",\n",
        "    \"metric\": \"auc\",\n",
        "    \"scale_pos_weight\": scale_pos_weight,\n",
        "    \"n_estimators\": 1000,\n",
        "    \"verbosity\": -1,\n",
        "    \"n_jobs\": -1,\n",
        "    \"force_col_wise\": True\n",
        "})\n",
        "print(\"Best parameters:\", best_params)\n"
      ],
      "metadata": {
        "id": "0iGqtKbJKQT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 18 â€” Markdown\n",
        "\n",
        "### 5.2 Training Final Model with Best Hyperparameters\n",
        "We now train the final LightGBM model using the `best_params` found from Optuna (Cell 17).\n",
        "This model will be trained on the full training set and validated on the hold-out validation set.\n",
        "We will also apply early stopping to avoid overfitting and save the model for later evaluation."
      ],
      "metadata": {
        "id": "gLV6hMl0OaHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 19 â€” Code\n",
        "# Train final LightGBM model using best hyperparameters\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score, classification_report\n",
        "\n",
        "# Prepare full train + calibration as training data\n",
        "X_fulltrain = pd.concat([X_train, X_cal])\n",
        "y_fulltrain = pd.concat([y_train, y_cal])\n",
        "\n",
        "dtrain = lgb.Dataset(X_fulltrain, label=y_fulltrain)\n",
        "dval   = lgb.Dataset(X_val, label=y_val)\n",
        "\n",
        "final_model = lgb.train(\n",
        "    best_params,\n",
        "    dtrain,\n",
        "    valid_sets=[dval],\n",
        "    num_boost_round=5000,\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=50),\n",
        "               lgb.log_evaluation(100)]\n",
        ")\n",
        "\n",
        "# Predictions on validation\n",
        "val_preds = final_model.predict(X_val)\n",
        "\n",
        "# Metrics\n",
        "roc_auc = roc_auc_score(y_val, val_preds)\n",
        "pr_auc  = average_precision_score(y_val, val_preds)\n",
        "\n",
        "print(f\"Validation ROC-AUC: {roc_auc:.6f}\")\n",
        "print(f\"Validation PR-AUC : {pr_auc:.6f}\")\n",
        "\n",
        "# Optional: convert probabilities to binary predictions (threshold = 0.5 for now)\n",
        "val_pred_labels = (val_preds > 0.5).astype(int)\n",
        "print(\"\\nClassification Report (threshold=0.5):\\n\")\n",
        "print(classification_report(y_val, val_pred_labels, digits=4))\n",
        "\n",
        "# Save model\n",
        "final_model.save_model(\"final_fraud_lgb.txt\")\n",
        "print(\"Final model saved as final_fraud_lgb.txt\")\n"
      ],
      "metadata": {
        "id": "EFknh2RBL7ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 20 â€” Markdown\n",
        "\n",
        "7) Key Predictors of Fraudulent Transactions\n",
        "\n",
        "Using feature importance from the final LightGBM model, we identify the variables that contribute most to fraud detection.\n",
        "\n",
        "Transaction Amount (amount, amt_log1p) â€“ Fraudulent transactions tend to have abnormal amounts (either very high or unusual patterns compared to userâ€™s history).\n",
        "\n",
        "Balance Changes (deltaOrig, deltaDest, balOrg_log1p, balDest_log1p) â€“ Large discrepancies between old and new balances are strong fraud indicators.\n",
        "\n",
        "Transaction Type (isTransfer, isCashOut) â€“ Fraud is more likely in transfers and cash-out operations than in payments.\n",
        "\n",
        "Time Features (hour, day) â€“ Fraud often clusters at unusual times of day (e.g., late night, off-peak hours).\n",
        "\n",
        "Destination Characteristics (destIsMerchant) â€“ Fraudsters sometimes avoid merchant accounts and send money to personal or â€œmuleâ€ accounts.\n",
        "\n",
        "Why these factors make sense:\n",
        "\n",
        "Fraud typically involves moving large sums rapidly across accounts with little prior balance.\n",
        "\n",
        "Transactions outside normal behavioral patterns (e.g., emptying account, odd hours) strongly suggest suspicious activity.\n",
        "\n",
        "Fraud is rarely linked to legitimate merchant transactions, reinforcing the importance of destIsMerchant.\n",
        "\n",
        "This provides the business with actionable signals: monitoring large transfers, balance mismatches, and suspicious time windows can help in real-time fraud prevention."
      ],
      "metadata": {
        "id": "ByCe1RMORjyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 21 â€” Code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = final_model.feature_importance(importance_type='gain')\n",
        "features = X_train.columns\n",
        "\n",
        "fi_df = pd.DataFrame({\n",
        "    \"feature\": features,\n",
        "    \"importance\": feature_importances\n",
        "}).sort_values(by=\"importance\", ascending=False)\n",
        "\n",
        "print(\"Top 10 Important Features:\\n\", fi_df.head(10))\n",
        "\n",
        "# Plot top 15\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(\n",
        "    data=fi_df.head(15),\n",
        "    x=\"importance\", y=\"feature\",\n",
        "    palette=\"viridis\"\n",
        ")\n",
        "plt.title(\"Top 15 Feature Importances (LightGBM)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1iPid1G3R2B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 22 â€” Markdown:\n",
        "\n",
        "7) Feature Importance Insights\n",
        "\n",
        "From the LightGBM feature importance analysis (Cell 21), we observe:\n",
        "\n",
        "amt_to_org_bal (transaction amount relative to origin account balance) is the strongest predictor of fraud, by a very large margin.\n",
        "\n",
        "amount (absolute transaction size) is also a strong factor.\n",
        "\n",
        "oldbalanceDest (balance of the destination account before the transaction) contributes meaningfully.\n",
        "\n",
        "Engineered log-transformed features (amt_log1p, balDest_log1p, etc.) and balance differences (deltaOrig, deltaDest) show moderate importance.\n",
        "\n",
        "Time-based variables like hour and day add signal, though less dominant.\n",
        "\n",
        "Transaction type indicators (isTransfer, isPayment, etc.) are less important, but still provide useful discrimination.\n",
        "\n",
        "Business Interpretation:\n",
        "\n",
        "Fraudsters tend to move unusually large proportions of available balances (high amt_to_org_bal).\n",
        "\n",
        "Certain destination account behaviors (oldbalanceDest) and timing patterns (hours of activity) help separate fraud vs. genuine transactions.\n",
        "\n",
        "These insights make sense because fraud often involves emptying accounts quickly, targeting certain merchants/accounts, and exploiting unusual timing.\n",
        "\n",
        "Next Step:\n",
        "We can use these insights to:\n",
        "\n",
        "Explain model behavior (address Candidate Expectation Q5 & Q6).\n",
        "\n",
        "Inform prevention strategies (e.g., flagging high-ratio amt_to_org_bal transactions in real-time)."
      ],
      "metadata": {
        "id": "fiTuNVp2SUEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 23 â€” Code\n",
        "import shap\n",
        "\n",
        "# Initialize SHAP explainer for LightGBM\n",
        "explainer = shap.TreeExplainer(final_model)\n",
        "shap_values = explainer.shap_values(X_val.sample(2000, random_state=42))  # sample for speed\n",
        "\n",
        "# Summary plot (global view of feature impact)\n",
        "shap.summary_plot(shap_values, X_val.sample(2000, random_state=42), plot_type=\"bar\")\n",
        "\n",
        "# Force plot for individual predictions (local interpretability)\n",
        "# Example: first transaction in the validation set\n",
        "shap.initjs()\n",
        "shap.force_plot(\n",
        "    explainer.expected_value,\n",
        "    shap_values[0,:],\n",
        "    X_val.sample(2000, random_state=42).iloc[0,:]\n",
        ")\n"
      ],
      "metadata": {
        "id": "J3pTrT87R4PK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 24 â€” Markdown\n",
        "\n",
        "### 7) Model Interpretability with SHAP\n",
        "\n",
        "To ensure the fraud detection model is not treated as a \"black box,\" we applied **SHAP (SHapley Additive exPlanations)** to explain both global and local predictions.\n",
        "\n",
        "**Global Interpretability (Summary Plot):**\n",
        "- `amt_to_org_bal` (transaction amount relative to origin balance) is by far the most influential feature.\n",
        "- `deltaOrig` (change in origin account balance) and `oldbalanceDest` also strongly affect predictions.\n",
        "- Transaction type indicators (`type_code`, `isPayment`, `isTransfer`) and temporal features (`hour`, `day`) add meaningful but smaller contributions.\n",
        "\n",
        "**Local Interpretability (Force Plot Example):**\n",
        "- For an individual transaction, SHAP identifies which features push the prediction **towards fraud (red)** or **towards non-fraud (blue)**.\n",
        "- Example: a transfer (`isTransfer=1`) with large balance discrepancies increases the fraud score, while a very large `amt_to_org_bal` pulls the score down toward non-fraud.\n",
        "\n",
        "**Business Value:**\n",
        "- Provides transparency in model decision-making.\n",
        "- Enables analysts to understand *why* a transaction is flagged.\n",
        "- Supports targeted prevention strategies by showing which risk factors drive alerts.\n"
      ],
      "metadata": {
        "id": "X4kcrpk4TLCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 25 â€” Business Insights from SHAP (fixed alignment)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shap\n",
        "\n",
        "# 1) Make ONE consistent validation sample (features + labels)\n",
        "N_SAMPLE = min(2000, len(X_val))\n",
        "X_val_sample = X_val.sample(N_SAMPLE, random_state=42)\n",
        "y_val_sample = y_val.loc[X_val_sample.index]\n",
        "\n",
        "# 2) Compute SHAP values on the same sample\n",
        "explainer = shap.TreeExplainer(final_model)\n",
        "raw_shap = explainer.shap_values(X_val_sample)\n",
        "\n",
        "# Handle SHAP output shape:\n",
        "# - Some versions return an array (n, features) for binary\n",
        "# - Others return a list [shap_neg, shap_pos]; we take the positive class\n",
        "if isinstance(raw_shap, list):\n",
        "    shap_values = raw_shap[1]\n",
        "else:\n",
        "    shap_values = raw_shap  # already (n, features)\n",
        "\n",
        "# 3) Build SHAP DataFrame aligned to the sample\n",
        "shap_df = pd.DataFrame(shap_values, columns=X_val_sample.columns, index=X_val_sample.index)\n",
        "shap_df[\"isFraud\"] = y_val_sample.values\n",
        "\n",
        "# 4) Global influence (mean |SHAP| across all sampled rows)\n",
        "global_shap_importance = shap_df.drop(\"isFraud\", axis=1).abs().mean().sort_values(ascending=False)\n",
        "print(\"Top 10 features influencing fraud decisions (by SHAP magnitude):\")\n",
        "print(global_shap_importance.head(10))\n",
        "\n",
        "# 5) Fraud-only influence (mean |SHAP| only on isFraud==1 rows)\n",
        "fraud_only = shap_df[shap_df[\"isFraud\"] == 1]\n",
        "if len(fraud_only) > 0:\n",
        "    fraud_feature_influence = fraud_only.drop(\"isFraud\", axis=1).abs().mean().sort_values(ascending=False)\n",
        "    print(\"\\nTop 10 features driving fraud transactions (by SHAP magnitude):\")\n",
        "    print(fraud_feature_influence.head(10))\n",
        "else:\n",
        "    print(\"\\n(No fraud rows in the sample; increase N_SAMPLE to include positives.)\")\n",
        "\n",
        "# 6) Average SHAP influence by transaction type (no column overlap)\n",
        "if \"type_code\" in X_val_sample.columns:\n",
        "    tmp = shap_df.drop(\"isFraud\", axis=1).copy()\n",
        "    tmp[\"type_code\"] = X_val_sample[\"type_code\"].values  # aligned by index\n",
        "    type_shap = (\n",
        "        tmp.groupby(\"type_code\")\n",
        "           .mean()                # average SHAP per feature within each type\n",
        "           .abs()\n",
        "           .mean(axis=1)          # average magnitude across features\n",
        "           .sort_values(ascending=False)\n",
        "    )\n",
        "    print(\"\\nAverage SHAP influence by transaction type:\")\n",
        "    print(type_shap)\n"
      ],
      "metadata": {
        "id": "dqlEdgYBTVjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper: map type_code back to labels\n",
        "# Works because we created `type_code` from df[\"type\"].cat.codes\n",
        "type_labels = list(df[\"type\"].cat.categories)\n",
        "code_to_label = {i: lbl for i, lbl in enumerate(type_labels)}\n",
        "print(\"type_code â†’ label mapping:\", code_to_label)\n",
        "\n",
        "# Apply mapping to your last table (replace indices with labels)\n",
        "type_shap_labeled = pd.Series(type_shap.index.map(code_to_label).values, index=type_shap.index)\n",
        "type_shap_labeled = pd.DataFrame({\"type\": type_shap.index.map(code_to_label), \"avg_shap\": type_shap.values})\n",
        "print(\"\\nAverage SHAP influence by transaction type (labeled):\")\n",
        "print(type_shap_labeled.sort_values(\"avg_shap\", ascending=False))\n"
      ],
      "metadata": {
        "id": "etejECB2dAMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 26 â€” Markdown\n",
        "### 6) Business Insights & Recommendations\n",
        "\n",
        "**Key Insights (Model + SHAP):**  \n",
        "- **`amt_to_org_bal`** (amount Ã· origin balance) is the dominant fraud driver â€” frauds are disproportionately large vs. available balance.  \n",
        "- **`deltaOrig`** and **`oldbalanceDest`** are strong: unusual balance drops at origin and atypically large destination balances correlate with fraud.  \n",
        "- **Transaction types (from `type_code`) ranked by fraud influence:**  \n",
        "  1) **TRANSFER** (highest) â†’ funds quickly leave the ecosystem  \n",
        "  2) **PAYMENT** â†’ suspicious merchants / â€œfrontsâ€  \n",
        "  3) **CASH_OUT** â†’ liquidation step in fraud chains  \n",
        "  4) **DEBIT**  \n",
        "  5) **CASH_IN** (lowest)  \n",
        "- Additional contributors: **`destIsMerchant`**, **`amount`**, and timing (**`hour`**).\n",
        "\n",
        "**Do these factors make sense? (Q6)**  \n",
        "âœ… Yes. Typical account-takeover and mule flows: **TRANSFER â†’ CASH_OUT**, often draining balances (high `amt_to_org_bal`), creating **balance inconsistencies** (`deltaOrig`, `deltaDest`), and routing to risky recipients (low merchant history or unusual `oldbalanceDest`). Payments can involve **front merchants**.\n",
        "\n",
        "**Recommendations (Q7) â€” Prevention & Infra Updates:**  \n",
        "1. **Real-time scoring & rules stack**  \n",
        "   - Deploy this model behind a low-latency API.  \n",
        "   - **Dynamic rule:** flag when `amt_to_org_bal > 0.8` (tune threshold) or when `|deltaOrig|` is anomalously large.  \n",
        "   - **Type-aware controls:** stricter thresholds for **TRANSFER/CASH_OUT** than PAYMENT/DEBIT/CASH_IN.\n",
        "2. **Step-up authentication**  \n",
        "   - For high-risk scores or high-value TRANSFER/CASH_OUT: enforce OTP/2FA, device binding, or biometric re-auth.\n",
        "3. **Recipient risk & velocity**  \n",
        "   - Maintain **recipient reputation** (age of account, first-seen, prior disputes).  \n",
        "   - Velocity features: #tx per hour/day per origin/dest; rapid emptying patterns.\n",
        "4. **Hold-and-review workflow**  \n",
        "   - Soft-hold top **X% risk**; triage queue with SHAP explanations to guide analysts.\n",
        "5. **Data & model ops**  \n",
        "   - Daily feature/label freshness checks; drift monitors on `amt_to_org_bal`, `delta*`, and type mix.  \n",
        "   - Weekly/biweekly retraining or when drift alarms trigger.\n",
        "\n",
        "**Measuring Effectiveness (Q8):**  \n",
        "- **Model KPIs:** PR-AUC, Recall (fraud capture), Precision (review load), Calibration (Brier).  \n",
        "- **Business KPIs:** Fraud loss prevented âˆ’ review cost, customer friction (false positive rate), decision latency (p95).  \n",
        "- **Experiment design:**  \n",
        "  - **A/B rollout** (model+rules vs. baseline rules).  \n",
        "  - **Threshold sweeps** to choose operating point by cost (missed fraud vs. review cost).  \n",
        "  - **Backtesting** on historical streams and **shadow mode** before full enforcement.\n",
        "\n",
        "**Traceability & Governance:**  \n",
        "- Keep **feature importance & SHAP** artifacts per version for auditability.  \n",
        "- Log model inputs/outputs, reason codes (top SHAP features), and analyst outcomes to close the feedback loop.\n"
      ],
      "metadata": {
        "id": "38FqQvkhTtTg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 27 â€” Markdown\n",
        "## 8) Conclusion & Case Answers (Q1â€“Q8)\n",
        "\n",
        "**Q1. Data cleaning (missing, outliers, multicollinearity)**  \n",
        "- No missing values.  \n",
        "- Outliers handled via winsorization/log transforms on amount/balances.  \n",
        "- Multicollinearity addressed by dropping redundant balances (`newbalanceOrig`, `newbalanceDest`) and using engineered deltas/ratios.\n",
        "\n",
        "**Q2. Fraud detection model (design & rationale)**  \n",
        "- Tuned **LightGBM** with **class weighting** (`scale_pos_weight â‰ˆ 774`) for extreme imbalance.  \n",
        "- Train/Cal/Val pipeline with early stopping; Optuna search on PR-AUC; probability-based evaluation.\n",
        "\n",
        "**Q3. Variable selection (how chosen)**  \n",
        "- **Domain-driven** (transaction type, balance deltas/ratios, timing).  \n",
        "- **Filter** (VIF to remove redundant features).  \n",
        "- **Embedded** (LightGBM gain + SHAP to confirm contribution and direction).\n",
        "\n",
        "**Q4. Performance demonstration**  \n",
        "- **Validation ROC-AUC â‰ˆ 0.9998**, **PR-AUC â‰ˆ 0.90**.  \n",
        "- High recall for fraud with acceptable precision (appropriate for fraud ops).  \n",
        "- Calibration/threshold tuning can trade precision vs. recall by business cost.\n",
        "\n",
        "**Q5. Key factors predicting fraud**  \n",
        "- `amt_to_org_bal` (dominant), `deltaOrig`, `oldbalanceDest`, `type_code` (TRANSFER/CASH_OUT), `destIsMerchant`, `amount`, time (`hour`).\n",
        "\n",
        "**Q6. Do these factors make sense?**  \n",
        "âœ… Yes. They reflect classic ATO/mule flows (large proportionate transfers, balance inconsistencies, risky recipients, liquidation via cash-out, off-hour activity).\n",
        "\n",
        "**Q7. Prevention during infra updates**  \n",
        "- Real-time scoring API + dynamic rules (e.g., high `amt_to_org_bal`, anomalous `delta*`).  \n",
        "- Step-up auth for high-risk TRANSFER/CASH_OUT.  \n",
        "- Recipient reputation & velocity checks; hold-and-review with SHAP reason codes.  \n",
        "- MLOps: drift monitors, retraining cadence, auditability.\n",
        "\n",
        "**Q8. How to determine if actions work**  \n",
        "- Track **PR-AUC/Recall/Precision**, fraud loss prevented vs. review cost, false-positive rate, decision latency.  \n",
        "- **A/B or phased rollout**, threshold sweeps, shadow mode, and backtesting on historical streams.\n",
        "\n",
        "**Artifacts saved:** final model (`final_fraud_lgb.txt`), evaluation metrics, feature importance & SHAP analyses for transparency and governance.\n"
      ],
      "metadata": {
        "id": "PakBMAIEeDtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 28 â€” Code (self-contained export of metrics, FI, and SHAP summaries)\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shap\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# -------- 1) Save final metrics (assumes val_preds, y_val, roc_auc, pr_auc exist from Cell 19) --------\n",
        "# If val_preds / metrics aren't in memory, recompute quickly:\n",
        "try:\n",
        "    _ = val_preds\n",
        "except NameError:\n",
        "    val_preds = final_model.predict(X_val, num_iteration=getattr(final_model, \"best_iteration\", None))\n",
        "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "    roc_auc = roc_auc_score(y_val, val_preds)\n",
        "    pr_auc  = average_precision_score(y_val, val_preds)\n",
        "\n",
        "final_metrics = {\n",
        "    \"roc_auc_val\": float(roc_auc),\n",
        "    \"pr_auc_val\": float(pr_auc),\n",
        "    \"classification_report\": classification_report(\n",
        "        y_val, (val_preds >= 0.5).astype(int), output_dict=True\n",
        "    ),\n",
        "    \"best_params\": {k: (float(v) if hasattr(v, \"__float__\") else v) for k, v in best_params.items()}\n",
        "}\n",
        "\n",
        "with open(\"final_metrics.json\", \"w\") as f:\n",
        "    json.dump(final_metrics, f, indent=2)\n",
        "\n",
        "# -------- 2) Save LightGBM feature importance --------\n",
        "fi = pd.DataFrame({\n",
        "    \"feature\": final_model.feature_name(),\n",
        "    \"importance_gain\": final_model.feature_importance(importance_type=\"gain\"),\n",
        "    \"importance_split\": final_model.feature_importance(importance_type=\"split\"),\n",
        "}).sort_values(by=\"importance_gain\", ascending=False)\n",
        "fi.to_csv(\"feature_importance.csv\", index=False)\n",
        "\n",
        "# -------- 3) Recompute SHAP on a consistent sample and export summaries --------\n",
        "N_SAMPLE = min(2000, len(X_val))\n",
        "X_val_sample = X_val.sample(N_SAMPLE, random_state=42)\n",
        "y_val_sample = y_val.loc[X_val_sample.index]\n",
        "\n",
        "explainer = shap.TreeExplainer(final_model)\n",
        "raw_shap = explainer.shap_values(X_val_sample)\n",
        "\n",
        "# Handle binary variants\n",
        "if isinstance(raw_shap, list):\n",
        "    shap_values = raw_shap[1]\n",
        "else:\n",
        "    shap_values = raw_shap\n",
        "\n",
        "# SHAP DataFrame aligned to the sample\n",
        "shap_df = pd.DataFrame(shap_values, columns=X_val_sample.columns, index=X_val_sample.index)\n",
        "shap_df[\"isFraud\"] = y_val_sample.values\n",
        "\n",
        "# Global SHAP importance (mean |SHAP| across all rows)\n",
        "global_shap_importance = (\n",
        "    shap_df.drop(columns=[\"isFraud\"]).abs().mean().sort_values(ascending=False)\n",
        "    .rename(\"mean_abs_shap\")\n",
        "    .to_frame()\n",
        "    .reset_index()\n",
        "    .rename(columns={\"index\": \"feature\"})\n",
        ")\n",
        "\n",
        "# Fraud-only SHAP importance\n",
        "fraud_only = shap_df[shap_df[\"isFraud\"] == 1]\n",
        "if len(fraud_only) > 0:\n",
        "    fraud_feature_influence = (\n",
        "        fraud_only.drop(columns=[\"isFraud\"]).abs().mean().sort_values(ascending=False)\n",
        "        .rename(\"mean_abs_shap_fraud_only\")\n",
        "        .to_frame()\n",
        "        .reset_index()\n",
        "        .rename(columns={\"index\": \"feature\"})\n",
        "    )\n",
        "else:\n",
        "    fraud_feature_influence = pd.DataFrame(columns=[\"feature\", \"mean_abs_shap_fraud_only\"])\n",
        "\n",
        "# Type-wise SHAP influence (if type_code exists)\n",
        "if \"type_code\" in X_val_sample.columns:\n",
        "    tmp = shap_df.drop(columns=[\"isFraud\"]).copy()\n",
        "    tmp[\"type_code\"] = X_val_sample[\"type_code\"].values\n",
        "    type_shap = (\n",
        "        tmp.groupby(\"type_code\").mean().abs().mean(axis=1).sort_values(ascending=False)\n",
        "        .rename(\"avg_mean_abs_shap\")\n",
        "        .to_frame()\n",
        "        .reset_index()\n",
        "    )\n",
        "    # Optional: add human-readable labels if categorical mapping is available\n",
        "    try:\n",
        "        type_labels = list(df[\"type\"].cat.categories)\n",
        "        code_to_label = {i: lbl for i, lbl in enumerate(type_labels)}\n",
        "        type_shap[\"type\"] = type_shap[\"type_code\"].map(code_to_label)\n",
        "        shap_type_influence = type_shap[[\"type_code\", \"type\", \"avg_mean_abs_shap\"]]\n",
        "    except Exception:\n",
        "        shap_type_influence = type_shap\n",
        "else:\n",
        "    shap_type_influence = pd.DataFrame(columns=[\"type_code\", \"avg_mean_abs_shap\"])\n",
        "\n",
        "# Save SHAP summaries\n",
        "global_shap_importance.to_csv(\"shap_feature_importance.csv\", index=False)\n",
        "fraud_feature_influence.to_csv(\"shap_fraud_only.csv\", index=False)\n",
        "shap_type_influence.to_csv(\"shap_type_influence.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Saved:\")\n",
        "print(\" - final_metrics.json\")\n",
        "print(\" - feature_importance.csv\")\n",
        "print(\" - shap_feature_importance.csv\")\n",
        "print(\" - shap_fraud_only.csv\")\n",
        "print(\" - shap_type_influence.csv\")\n"
      ],
      "metadata": {
        "id": "vBuq2on4TXpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 29 â€” Code (Download artifacts from Colab)\n",
        "from google.colab import files\n",
        "for fname in [\n",
        "    \"final_fraud_lgb.txt\",\n",
        "    \"final_metrics.json\",\n",
        "    \"feature_importance.csv\",\n",
        "    \"shap_feature_importance.csv\",\n",
        "    \"shap_fraud_only.csv\",\n",
        "    \"shap_type_influence.csv\",\n",
        "]:\n",
        "    try:\n",
        "        files.download(fname)\n",
        "    except Exception as e:\n",
        "        print(f\"Skip {fname}: {e}\")"
      ],
      "metadata": {
        "id": "cxNB-Vs4grJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 30 â€” Code (Optional: cost-sensitive threshold tuning + export predictions)\n",
        "\n",
        "# This finds a business-aware threshold (min cost) and saves a predictions file.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_curve\n",
        "\n",
        "# Make sure we have probabilities from the final model\n",
        "val_proba = final_model.predict(X_val, num_iteration=getattr(final_model, \"best_iteration\", None))\n",
        "\n",
        "# Example business costs (tune for your org)\n",
        "REVIEW_COST = 5.0     # $ per false positive (manual review)\n",
        "LOSS_FACTOR = 0.7     # 70% of fraudulent amount lost if missed\n",
        "\n",
        "# We need amounts for the validation set to estimate FN cost\n",
        "val_amounts = df.loc[X_val.index, \"amount\"].values\n",
        "\n",
        "def expected_cost(y_true, proba, thr):\n",
        "    y_pred = (proba >= thr).astype(int)\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
        "    TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "    fp_cost = FP * REVIEW_COST\n",
        "    fn_cost = (val_amounts[(y_true==1) & (y_pred==0)] * LOSS_FACTOR).sum()\n",
        "    return fp_cost + fn_cost, cm\n",
        "\n",
        "grid = np.linspace(0.01, 0.99, 99)\n",
        "costs, f1s = [], []\n",
        "for t in grid:\n",
        "    c, cm = expected_cost(y_val.values, val_proba, t)\n",
        "    costs.append(c)\n",
        "    f1s.append(f1_score(y_val, (val_proba>=t).astype(int)))\n",
        "\n",
        "best_cost_idx = int(np.argmin(costs))\n",
        "best_thr_cost = float(grid[best_cost_idx])\n",
        "best_cost = float(costs[best_cost_idx])\n",
        "best_f1_thr = float(grid[int(np.argmax(f1s))])\n",
        "\n",
        "print(f\"Best cost threshold: {best_thr_cost:.3f} | Estimated cost: {best_cost:,.2f}\")\n",
        "print(f\"Best F1 threshold  : {best_f1_thr:.3f} | F1: {max(f1s):.4f}\")\n",
        "\n",
        "# Export validation predictions at the chosen threshold\n",
        "chosen_thr = best_thr_cost  # or best_f1_thr\n",
        "pred_df = pd.DataFrame({\n",
        "    \"proba_fraud\": val_proba,\n",
        "    \"pred_fraud\": (val_proba >= chosen_thr).astype(int),\n",
        "    \"true_fraud\": y_val.values\n",
        "}, index=X_val.index)\n",
        "pred_df.to_csv(\"validation_predictions.csv\")\n",
        "print(\"Saved validation_predictions.csv with threshold =\", chosen_thr)"
      ],
      "metadata": {
        "id": "W4A0F9cAgtGY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}